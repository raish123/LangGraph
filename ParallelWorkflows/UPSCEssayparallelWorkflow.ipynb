{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6799f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings #this to above classes we used to interact with llm model.\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph,END,START #this class we used to build stateful workflows in graphical form mei.\n",
    "from pydantic import BaseModel,Field,computed_field\n",
    "from typing import Annotated,List,Dict,TypedDict,Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b332d",
   "metadata": {},
   "source": [
    "# step1) define the LLM model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4606ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we have to define kon se HuggingFaceEndpoint pe API Request jayeghi\n",
    "# Hugging Face endpoint define karo\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Chat model object banao\n",
    "model1 = ChatHuggingFace(llm=llm)\n",
    "model2 = ChatOpenAI(temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbf9ff",
   "metadata": {},
   "source": [
    "# defining structure output by using pydantic class this will return struture result from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9112e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "#structure schema defining to get response from LLM in this Way.\n",
    "class EssayFeedbackScore(BaseModel):\n",
    "    feedback: Annotated[\n",
    "        Literal[\n",
    "            \"Clarity of thought\",\n",
    "            \"Depth of analysis\",\n",
    "            \"Language quality\"\n",
    "        ],\n",
    "        str\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Category of essay feedback: clarity of thought, depth of analysis, or language quality.\"\n",
    "    )\n",
    "\n",
    "    score: Annotated[\n",
    "        int,\n",
    "        Field(..., description=\"Score for the essay on a scale of 1 to 10 based on the selected feedback category.\",ge=0,le=10)\n",
    "    ]\n",
    "    \n",
    "    explanation: Annotated[\n",
    "        str,\n",
    "        Field(..., description=\"A Detailed Feedback For The Essay.\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5490668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.EssayFeedbackScore'>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating an object of pydantic output parser class\n",
    "parser  = StrOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=EssayFeedbackScore)\n",
    "pydantic_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9125047",
   "metadata": {},
   "source": [
    "# testing pydantic parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a4089d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n    You are an expert evaluator of essays. \\n    Read the essay below and evaluate it based on **one** of the following categories only:\\n    - Clarity of thought\\n    - Depth of analysis\\n    - Language quality\\n\\n    Then:\\n    - Select the most relevant feedback category.\\n    - Assign a score between 0 and 10.\\n    - Provide a short detailed explanation justifying the score.\\n\\n    Essay:\\n    Artificial Intelligence (AI) is transforming the world, and India is embracing this technology to solve real challenges. In healthcare, AI is helping doctors with faster diagnosis and better treatment decisions. Artificial intelligence is very important in world today.\\nIt help many peoples to finish work fast.\\nStudent also use AI for write homework and essay.\\nSometime teacher not happy because student not learning.\\nCompany like to use AI because machine not take rest.\\nBut many worker afraid they job going lost.\\nAI also make mistake because it not have human mind.\\nSome people say AI is very danger for future.\\nOther people think it is only tool for help.\\nSo AI is good and bad both together. The future of AI in India is both promising and impactful.\\n\\n    Return the result strictly following this format:\\n    The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"feedback\": {\"description\": \"Category of essay feedback: clarity of thought, depth of analysis, or language quality.\", \"enum\": [\"Clarity of thought\", \"Depth of analysis\", \"Language quality\"], \"title\": \"Feedback\", \"type\": \"string\"}, \"score\": {\"description\": \"Score for the essay on a scale of 1 to 10 based on the selected feedback category.\", \"maximum\": 10, \"minimum\": 0, \"title\": \"Score\", \"type\": \"integer\"}, \"explanation\": {\"description\": \"A Detailed Feedback For The Essay.\", \"title\": \"Explanation\", \"type\": \"string\"}}, \"required\": [\"feedback\", \"score\", \"explanation\"]}\\n```\\n    ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert evaluator of essays. \n",
    "    Read the essay below and evaluate it based on **one** of the following categories only:\n",
    "    - Clarity of thought\n",
    "    - Depth of analysis\n",
    "    - Language quality\n",
    "\n",
    "    Then:\n",
    "    - Select the most relevant feedback category.\n",
    "    - Assign a score between 0 and 10.\n",
    "    - Provide a short detailed explanation justifying the score.\n",
    "\n",
    "    Essay:\n",
    "    {essay}\n",
    "\n",
    "    Return the result strictly following this format:\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=['essay'],\n",
    "    partial_variables={'format_instructions': pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "with open('essay.txt','r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "#want to see the prompt.\n",
    "prompt.invoke({'essay':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5c6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing this prompt to model\n",
    "# chain = prompt | model1 | pydantic_parser\n",
    "\n",
    "# response = chain.invoke({'essay':text})\n",
    "# response\n",
    "\n",
    "#response.explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5f35f",
   "metadata": {},
   "source": [
    "# step:1) defining the state or Memory Field using TypedDict or pydantic this state will be sharing throught workflow then creating graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be113355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "class UpscEssayState(BaseModel):\n",
    "    essay_txt: str = Field(..., description=\"Essay text provided by user\")\n",
    "    \n",
    "    COT_feedback: Optional[str] = Field(None, description=\"Feedback for Clarity of Thought\")\n",
    "    DOA_feedback: Optional[str] = Field(None, description=\"Feedback for Depth of Analysis\")\n",
    "    LQ_feedback: Optional[str] = Field(None, description=\"Feedback for Language Quality\")\n",
    "    \n",
    "    overall_feedback: Optional[str] = Field(None, description=\"Final summarized feedback\")\n",
    "    \n",
    "    # Reducer: collect all node scores in a list\n",
    "    individual_score_each_nodes: Annotated[\n",
    "        List[int], operator.add\n",
    "    ] = Field(default_factory=list, description=\"Scores returned by multiple nodes, combined via reducer\")\n",
    "    \n",
    "    final_score: Optional[float] = Field(None, ge=0, le=10, description=\"Final averaged score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1b9d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1b72faabf40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a graph object by using stategraph class.\n",
    "graph = StateGraph(UpscEssayState)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb4215",
   "metadata": {},
   "source": [
    "# step:2) now adding nodes or edges to the graph after then starting workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618c428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating evaluate_thought nodes ke liye user defined function and performing action init.\n",
    "def evaluate_thought(state:UpscEssayState) ->UpscEssayState:\n",
    "    \n",
    "    text_essay = state.essay_txt\n",
    "    \n",
    "    #Now this essay we change them to structured instruction Manner.\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert evaluator of essays. \n",
    "    Read the essay below and evaluate it strictly on **Clarity of Thought**.\n",
    "\n",
    "    Then:\n",
    "    - Assign a score between 0 and 10.\n",
    "    - Provide a short detailed explanation justifying the score.\n",
    "\n",
    "    Essay:\n",
    "    {essay}\n",
    "\n",
    "    Return the result strictly following this format:\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=['essay'],\n",
    "    partial_variables={'format_instructions': pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "    \n",
    "    \n",
    "    #now passing this prompt to the structure response from LLM using chains\n",
    "    chain = prompt | model2 | pydantic_parser\n",
    "    \n",
    "    #invoking the or calling the chain to get response from llm.\n",
    "    result = chain.invoke({'essay':text_essay})\n",
    "    \n",
    "    #now updating the state with partial update and return back the updated state.\n",
    "    \n",
    "    return {\n",
    "            \"COT_feedback\":result.explanation, # explanation from LLM\n",
    "            \"individual_score_each_nodes\": [result.score]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14c9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analysis(state:UpscEssayState) ->UpscEssayState:\n",
    "    text_essay = state.essay_txt\n",
    "    \n",
    "    #Now this essay we change them to structured instruction Manner.\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert evaluator of essays. \n",
    "    Read the essay below and evaluate it strictly on **Depth of Analysis**.\n",
    "\n",
    "   Then:\n",
    "    - Assign a score between 0 and 10.\n",
    "    - Provide a short detailed explanation justifying the score.\n",
    "    \n",
    "    Essay:\n",
    "    {essay}\n",
    "\n",
    "    Return the result strictly following this format:\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=['essay'],\n",
    "    partial_variables={'format_instructions': pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "    \n",
    "    \n",
    "    #now passing this prompt to the structure response from LLM using chains\n",
    "    chain = prompt | model2 | pydantic_parser\n",
    "    \n",
    "    #invoking the or calling the chain to get response from llm.\n",
    "    result = chain.invoke({'essay':text_essay})\n",
    "    \n",
    "    #now updating the state with partial update and return back the updated state.\n",
    "    \n",
    "    return {\n",
    "            \"DOA_feedback\":result.explanation, # explanation from LLM\n",
    "            \"individual_score_each_nodes\": [result.score]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f50f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_language(state:UpscEssayState) ->UpscEssayState:\n",
    "    text_essay = state.essay_txt\n",
    "    \n",
    "    #Now this essay we change them to structured instruction Manner.\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert evaluator of essays. \n",
    "    Read the essay below and evaluate it strictly on **Language Quality**.\n",
    "    \n",
    "    Then:\n",
    "    - Assign a score between 0 and 10.\n",
    "    - Provide a short detailed explanation justifying the score.\n",
    "    \n",
    "    Essay:\n",
    "    {essay}\n",
    "\n",
    "    Return the result strictly following this format:\n",
    "    {format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=['essay'],\n",
    "    partial_variables={'format_instructions': pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "    \n",
    "    \n",
    "    #now passing this prompt to the structure response from LLM using chains\n",
    "    chain = prompt | model2 | pydantic_parser\n",
    "    \n",
    "    #invoking the or calling the chain to get response from llm.\n",
    "    result = chain.invoke({'essay':text_essay})\n",
    "    \n",
    "    #now updating the state with partial update and return back the updated state.\n",
    "    \n",
    "    return {\n",
    "            \"LQ_feedback\":result.explanation, # explanation from LLM\n",
    "            \"individual_score_each_nodes\": [result.score]\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "883fdef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_essay(state:UpscEssayState) ->UpscEssayState:\n",
    "    cot = state.COT_feedback\n",
    "    doa = state.DOA_feedback\n",
    "    lq = state.LQ_feedback\n",
    "    individual_score = state.individual_score_each_nodes\n",
    "    \n",
    "    #calculating average score.\n",
    "    avg_score = sum(individual_score)/len(individual_score)\n",
    "    \n",
    "    #creating structure instruction prompt.\n",
    "    final_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an expert evaluator of essays. \n",
    "        Summarize the following feedback categories into a final review:\n",
    "        - Clarity of thought: {cot}\n",
    "        - Depth of analysis: {doa}\n",
    "        - Language quality: {lq}\n",
    "\n",
    "        Also consider the average score {scores:.2f} as the final score.\n",
    "        \n",
    "        Provide a well-structured overall feedback paragraph.\n",
    "        \"\"\",\n",
    "        input_variables=[\"cot\", \"doa\", \"lq\", \"scores\"]\n",
    ")\n",
    "    \n",
    "    #passing this prompt to LLM model forming chain.\n",
    "    chain = final_prompt | model2 | parser\n",
    "    \n",
    "\n",
    "    \n",
    "    #invoking the chain.\n",
    "    response_llm = chain.invoke({\"cot\":cot, \"doa\":doa, \"lq\":lq, \"scores\":avg_score})\n",
    "  \n",
    "    \n",
    "    #updating the state and returning the updated state.\n",
    "    return {\n",
    "        \n",
    "        'overall_feedback':response_llm,\n",
    "        \"final_score\": avg_score\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39e77b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1b72faabf40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding nodes to graph.\n",
    "#nodes is nothing but python function in which we r defing actions to perform.\n",
    "graph.add_node(node=\"evaluate_thought\",action=evaluate_thought)\n",
    "graph.add_node(node=\"evaluate_analysis\",action=evaluate_analysis)\n",
    "graph.add_node(node=\"evaluate_language\",action=evaluate_language)\n",
    "graph.add_node(node=\"final_evaluate_essay\",action=evaluate_final_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95efe728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1b72faabf40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now adding the edges to graph edges will connect the nodes whenever reqd we can set loop or condition to edge.\n",
    "graph.add_edge(START,'evaluate_thought')\n",
    "graph.add_edge(START,'evaluate_analysis')\n",
    "graph.add_edge(START,'evaluate_language')\n",
    "\n",
    "graph.add_edge('evaluate_thought','final_evaluate_essay')\n",
    "graph.add_edge('evaluate_analysis','final_evaluate_essay')\n",
    "graph.add_edge('evaluate_language','final_evaluate_essay')\n",
    "\n",
    "graph.add_edge('final_evaluate_essay',END)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb958fa8",
   "metadata": {},
   "source": [
    "# step:3) compiling the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59cf0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687c6f",
   "metadata": {},
   "source": [
    "# step:4) evaluating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56bf07e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpscEssayState(essay_txt='Artificial Intelligence (AI) is transforming the world, and India is embracing this technology to solve real challenges. In healthcare, AI is helping doctors with faster diagnosis and better treatment decisions. Artificial intelligence is very important in world today.\\nIt help many peoples to finish work fast.\\nStudent also use AI for write homework and essay.\\nSometime teacher not happy because student not learning.\\nCompany like to use AI because machine not take rest.\\nBut many worker afraid they job going lost.\\nAI also make mistake because it not have human mind.\\nSome people say AI is very danger for future.\\nOther people think it is only tool for help.\\nSo AI is good and bad both together. The future of AI in India is both promising and impactful.', COT_feedback=None, DOA_feedback=None, LQ_feedback=None, overall_feedback=None, individual_score_each_nodes=[], final_score=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = graph.compile()\n",
    "\n",
    "#passing this initial state to graph or starting point.\n",
    "intitial_state = UpscEssayState(\n",
    "    essay_txt=text\n",
    ")\n",
    "intitial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d31f79",
   "metadata": {},
   "source": [
    "# passing this initial to my parallel workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca400b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'essay_txt': 'Artificial Intelligence (AI) is transforming the world, and India is embracing this technology to solve real challenges. In healthcare, AI is helping doctors with faster diagnosis and better treatment decisions. Artificial intelligence is very important in world today.\\nIt help many peoples to finish work fast.\\nStudent also use AI for write homework and essay.\\nSometime teacher not happy because student not learning.\\nCompany like to use AI because machine not take rest.\\nBut many worker afraid they job going lost.\\nAI also make mistake because it not have human mind.\\nSome people say AI is very danger for future.\\nOther people think it is only tool for help.\\nSo AI is good and bad both together. The future of AI in India is both promising and impactful.',\n",
       " 'COT_feedback': \"The essay lacks clarity of thought as it jumps between different points without proper transitions or development. The ideas presented are fragmented and do not flow logically. The use of examples is limited and does not effectively support the main points. Overall, the essay lacks coherence and fails to clearly convey the writer's thoughts.\",\n",
       " 'DOA_feedback': 'The essay briefly mentions some points about the impact of AI in healthcare and education, but lacks in-depth analysis. It fails to provide detailed examples, explanations, or insights into the topic. The arguments presented are superficial and do not delve deep into the complexities of AI technology and its implications. Overall, the analysis is shallow and lacks critical thinking.',\n",
       " 'LQ_feedback': 'The essay lacks proper grammar, punctuation, and sentence structure. There are multiple spelling errors and the use of language is very basic. The essay does not flow well and lacks coherence. Overall, the language quality is poor.',\n",
       " 'overall_feedback': \"Overall, the essay falls short in several key areas. The lack of clarity of thought results in a disjointed and confusing presentation of ideas, making it difficult for the reader to follow the writer's train of thought. Additionally, the shallow analysis of the impact of AI in healthcare and education fails to provide a thorough exploration of the topic, leaving the reader with unanswered questions and a lack of depth. Furthermore, the poor language quality detracts from the overall readability and effectiveness of the essay. With improvements in organization, analysis, and language usage, the essay could greatly benefit from a more cohesive and insightful presentation. Considering the average score of 2.67, there is room for significant improvement in all aspects of the essay.\",\n",
       " 'individual_score_each_nodes': [2, 3, 3],\n",
       " 'final_score': 2.6666666666666665}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = workflow.invoke(intitial_state)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b59d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
