Bilkul! Main is code ko code by code samjhata hoon, line by line aur block by block. Har step ke baad explanation dunga.

âœ… 1. Install necessary libraries
# pip install -U langchain langchain-openai langchain-community faiss-cpu pypdf python-dotenv langsmith


âœ” Ye command Python libraries install karta hai.
âœ”

langchain â†’ workflows banane ke liye

langchain-openai â†’ OpenAI models se connect karne ke liye

langchain-community â†’ extra tools jaise PDF loader

faiss-cpu â†’ vector search engine

pypdf â†’ PDF files ko read karne ke liye

python-dotenv â†’ .env file se secret variables load karne ke liye

langsmith â†’ observability aur tracking ke liye

âœ… 2. Import statements
import os
import json
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from langsmith import traceable
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser


âœ” Yeh sab external aur built-in Python libraries hain.
âœ” traceable â†’ Langsmith ke liye observability decorator hai.
âœ” PyPDFLoader â†’ PDF file ko read karta hai.
âœ” RecursiveCharacterTextSplitter â†’ PDF ko chunks mein todta hai.
âœ” OpenAIEmbeddings â†’ text ko vector mein convert karta hai.
âœ” FAISS â†’ vector ko index aur search karne ke liye tool hai.
âœ” ChatPromptTemplate â†’ question aur context ka format define karta hai.
âœ” Runnable* â†’ processing pipeline ka structure banata hai.
âœ” StrOutputParser â†’ final output ko string format mein convert karta hai.

âœ… 3. Load environment variables
load_dotenv()


âœ” .env file se API keys aur configuration load karta hai.

âœ… 4. Set PDF file path and environment settings
PDF_PATH = "Langsmith_Tracing\islr.pdf"  # change to your file
os.environ["LANGCHAIN_PROJECT"] = "RagTracingEverything_EnsuringInitialLevel_didnt_ReRun"
parser = StrOutputParser()


âœ” PDF_PATH â†’ aap jis PDF ko process karna chahte hain uska path.
âœ” LANGCHAIN_PROJECT â†’ project ka naam set karta hai jo Langsmith dashboard mein dikhai dega.
âœ” StrOutputParser â†’ output ko string mein convert karega.

âœ… 5. Index folder create karna
INDEX_ROOT = Path(".indices")
INDEX_ROOT.mkdir(exist_ok=True)


âœ” Agar .indices folder nahi hai to ye bana deta hai.
âœ” Yahan vector indexes store honge.

âœ… 6. Helper functions â€“ load PDF, split, vectorstore
Load PDF
@traceable(name="load_pdf")
def load_pdf(path: str):
    return PyPDFLoader(path).load()


âœ” PDF ko read karta hai.
âœ” @traceable se ye function Langsmith mein trace hoga.

Split documents
@traceable(name="split_documents")
def split_documents(docs, chunk_size=1000, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_documents(docs)


âœ” PDF ke text ko chhote chunks mein todta hai.
âœ” Chunk size aur overlap define karta hai.

Build vectorstore
@traceable(name="build_vectorstore")
def build_vectorstore(splits, embed_model_name: str):
    emb = OpenAIEmbeddings(model=embed_model_name)
    return FAISS.from_documents(splits, emb)


âœ” Text chunks ko vector format mein convert karta hai using embeddings.
âœ” FAISS mein index banata hai.

âœ… 7. File fingerprinting â€“ cache key generate karna
File fingerprint
def _file_fingerprint(path: str) -> dict:
    p = Path(path)
    h = hashlib.sha256()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return {"sha256": h.hexdigest(), "size": p.stat().st_size, "mtime": int(p.stat().st_mtime)}


âœ” File ka hash, size, aur modified time nikalta hai.
âœ” Isse pata chalega agar file change hui hai.

Index key
def _index_key(pdf_path: str, chunk_size: int, chunk_overlap: int, embed_model_name: str) -> str:
    meta = {
        "pdf_fingerprint": _file_fingerprint(pdf_path),
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "embedding_model": embed_model_name,
        "format": "v1",
    }
    return hashlib.sha256(json.dumps(meta, sort_keys=True).encode("utf-8")).hexdigest()


âœ” File fingerprint aur settings ko mila kar ek unique key banata hai.
âœ” Agar file ya settings change hui to naye index generate honge.

âœ… 8. Load or build index run
Load index
@traceable(name="load_index", tags=["index"])
def load_index_run(index_dir: Path, embed_model_name: str):
    emb = OpenAIEmbeddings(model=embed_model_name)
    return FAISS.load_local(str(index_dir), emb, allow_dangerous_deserialization=True)


âœ” Agar index save hai to use load karta hai.

Build index
@traceable(name="build_index", tags=["index"])
def build_index_run(pdf_path: str, index_dir: Path, chunk_size: int, chunk_overlap: int, embed_model_name: str):
    docs = load_pdf(pdf_path)
    splits = split_documents(docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    vs = build_vectorstore(splits, embed_model_name)
    index_dir.mkdir(parents=True, exist_ok=True)
    vs.save_local(str(index_dir))
    (index_dir / "meta.json").write_text(json.dumps({
        "pdf_path": os.path.abspath(pdf_path),
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "embedding_model": embed_model_name,
    }, indent=2))
    return vs


âœ” Agar index nahi hai to naya index banata hai aur save karta hai.

Dispatcher â€“ decide whether to load or build
def load_or_build_index(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 150, embed_model_name: str = "text-embedding-3-small", force_rebuild: bool = False):
    key = _index_key(pdf_path, chunk_size, chunk_overlap, embed_model_name)
    index_dir = INDEX_ROOT / key
    cache_hit = index_dir.exists() and not force_rebuild
    if cache_hit:
        return load_index_run(index_dir, embed_model_name)
    else:
        return build_index_run(pdf_path, index_dir, chunk_size, chunk_overlap, embed_model_name)


âœ” Pehle check karta hai index available hai ya nahi.
âœ” Agar hai â†’ load karega
âœ” Agar nahi â†’ build karega

âœ… 9. Model, prompt, and pipeline setup
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer ONLY from the provided context. If not found, say you don't know."),
    ("human", "Question: {question}\n\nContext:\n{context}")
])


âœ” OpenAI ka model use kiya gaya hai.
âœ” Prompt structure define kiya hai â€“ context ke based answer dena hai.

def format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)


âœ” Retrieved documents ko concatenate karta hai.

âœ… 10. Pipeline functions with observability
@traceable(name="setup_pipeline", tags=["setup"])
def setup_pipeline(pdf_path: str, chunk_size=1000, chunk_overlap=150, embed_model_name="text-embedding-3-small", force_rebuild=False):
    return load_or_build_index(pdf_path=pdf_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap, embed_model_name=embed_model_name, force_rebuild=force_rebuild)


âœ” Index setup karta hai.

@traceable(name="pdf_rag_full_run")
def setup_pipeline_and_query(pdf_path: str, question: str, chunk_size: int = 1000, chunk_overlap: int = 150, embed_model_name: str = "text-embedding-3-small", force_rebuild: bool = False):
    vectorstore = setup_pipeline(pdf_path, chunk_size, chunk_overlap, embed_model_name, force_rebuild)
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})
    parallel = RunnableParallel({
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough(),
    })
    chain = parallel | prompt | llm | StrOutputParser()
    return chain.invoke(question, config={"run_name": "pdf_rag_query", "tags": ["qa"], "metadata": {"k": 4}})


âœ” Query pipeline create karta hai.
âœ” Similar documents search karta hai.
âœ” LLM se answer leta hai.
âœ” Observability trace karta hai.

âœ… 11. CLI â€“ Interactive interface
if __name__ == "__main__":
    print("PDF RAG ready. Ask a question (or Ctrl+C to exit).")
    q = input("\nQ: ").strip()
    ans = setup_pipeline_and_query(PDF_PATH, q)
    print("\nA:", ans)


âœ” Command-line par user se question leta hai.
âœ” Setup pipeline aur query run karta hai.
âœ” Answer print karta hai.

ðŸ“Œ Conclusion

âœ… Yeh code PDF ko process karke uska searchable knowledge base banata hai.

âœ… Langchain ka use karke text ko vector mein convert karta hai.

âœ… Langsmith se observability aur debugging ko track karta hai.

âœ… Efficient caching aur indexing system implement karta hai.

âœ… CLI se interactive QA system banata hai.

Agar chahein to next steps mein:

âœ” .env file ka setup karna
âœ” Langsmith dashboard par runs dekhna
âœ” Is code ko deploy karna
âœ” Observability features explore karna

bata sakte hain! Ready?